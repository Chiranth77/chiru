| Title | Methodology | Strengths | Limitations |
|-----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| **Federated Learning with CNNs: A Systematic Review (2023)** | This paper reviews various CNN architectures used in Federated Learning, covering their design, algorithmic improvements, and security vulnerabilities such as data leakage, poisoning attacks, and adversarial training. It highlights the security challenges that arise when CNNs are distributed across clients. | Provides a comprehensive understanding of the state-of-the-art CNNs in FL; gives a clear overview of current security concerns like privacy, model poisoning, and malicious attacks. | The paper doesn’t introduce new algorithms; it’s focused on summarizing existing work, with no experimental results provided. |
| **Federated Learning with CNNs for Medical Image Segmentation (2022)** | Proposes a CNN-based federated learning system for medical image segmentation, with an emphasis on preserving patient privacy through secure aggregation and encryption techniques. The system allows multiple hospitals to collaboratively train models without exchanging raw data. | Achieves high performance in medical image segmentation tasks while preserving the privacy of medical data; implements encryption to secure model updates and protect against data leakage. | The proposed approach focuses on a specific domain (medical imaging) and may not generalize well to other fields; lacks thorough evaluation of model robustness against adversarial attacks. |
| **Robust Federated Learning with CNNs Against Poisoning Attacks (2021)** | Proposes a federated learning framework that detects and mitigates poisoning attacks during CNN training. This is achieved using anomaly detection techniques to identify malicious clients and improve model robustness in distributed training environments. | Shows high robustness against both data and model poisoning attacks; improves accuracy and model security in collaborative learning scenarios involving many untrusted clients. | The method adds significant overhead to training time; limited evaluation of robustness against sophisticated adversarial attacks; might not be scalable for large federated systems. |
| **Securing CNN Models in Federated Learning Through Homomorphic Encryption (2020)** | Implements homomorphic encryption in CNN-based federated learning to protect model updates and training data. This ensures that participants’ data remain private while enabling collaborative model training. The system allows encrypted computation on data, thus avoiding exposure of raw inputs. | Ensures strong privacy protection through encryption while maintaining model performance; suitable for applications involving sensitive data, such as finance and healthcare. | The use of homomorphic encryption introduces significant computational overhead, which can slow down the federated learning process; may not scale efficiently for large datasets or complex models. |
| **A Blockchain-Based Framework for Federated Learning with CNNs (2022)** | Integrates blockchain technology into federated learning with CNNs to secure model updates and ensure the authenticity of the contributions. The blockchain serves as an immutable ledger for tracking the model update history, preventing tampering by malicious clients. | Combines blockchain and FL for tamper-proof, transparent model training; improves trust and security in decentralized training environments; well-suited for multi-party systems where authenticity is critical. | Blockchain integration increases computational and communication costs; the approach is resource-intensive and may not be viable for large-scale applications with limited resources. |
| **Federated Adversarial Learning with CNNs for Secure Model Training (2021)** | Combines adversarial learning techniques with CNNs in federated learning to improve model robustness against adversarial attacks. The framework involves generating adversarial examples on local clients and refining the global model to withstand these attacks. | Enhances model security by improving its resistance to adversarial attacks; provides better generalization across diverse federated clients; useful in hostile environments like IoT or cybersecurity. | Requires significant fine-tuning of hyperparameters; introduces additional computational costs due to adversarial training, which may hinder scalability. |
| **Differential Privacy for CNNs in Federated Learning (2021)** | Implements differential privacy mechanisms in CNN-based federated learning to protect the privacy of individual participants. By adding noise to model updates before aggregation, it ensures that no individual data point can be inferred from the final model. | Guarantees a strong level of privacy protection, making it suitable for applications involving sensitive data like healthcare or financial services; maintains high model performance. | Differential privacy introduces noise into the model, which may lower the accuracy; requires a careful trade-off between privacy and model performance; not well-suited for high-stakes applications where accuracy is paramount. |
| **Lightweight Federated Learning with CNNs for IoT Devices (2021)** | Proposes a lightweight federated learning framework using CNNs, designed specifically for resource-constrained IoT devices. It focuses on reducing communication and computational costs while ensuring secure model updates using encryption techniques. | Well-suited for edge computing and IoT environments with limited resources; reduces communication and resource usage while preserving security; efficient model update process with minimal latency. | May not be effective for large-scale data or highly complex models; performance may degrade in environments with large datasets or when using deep CNN architectures. |
